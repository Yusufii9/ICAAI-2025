# Knowledge-Driven Vision-Language Model for Plexus Detection in Hirschsprungâ€™s Disease
## ğŸ“‹ Overview

This project proposes a novel framework that integrates expert-derived textual concepts into a CLIP-based vision-language model to guide plexus classification in histopathological whole slide images (WSIs) for Hirschsprungâ€™s Disease (HD).  
Our method aims to combine the strengths of Vision Transformers (ViTs) with clinical knowledge, improving interpretability and alignment with real-world diagnostic processes.

## âœ¨ Highlights

- **Baseline Model:** Vision Transformer (ViT) fine-tuned on histopathological tiles.
- **Proposed Model:** CLIP-based multi-modal model integrating expert prompts.
- **Dataset:** 30 WSIs from 26 patients (Childrenâ€™s Hospital of Eastern Ontario). The dataset folder will be left empty, as this is a private dataset and cannot be shared publicly. 
- **Results:**  
  - ViT achieved higher raw classification accuracy (87.17%).  
  - Proposed CLIP model achieved better AUC (91.76%), showing stronger discriminative ability.
- **Contribution:** Demonstrates how expert knowledge can enhance model reasoning in medical imaging tasks.

## ğŸ“‚ Project Structure
```
KDVLM/
â”œâ”€â”€ ConcepPath/
â”œâ”€â”€ dataset/
â”œâ”€â”€ experiment/plexus_detection/
â”œâ”€â”€ prompts/
â”œâ”€â”€ reports/
â”œâ”€â”€ saved_rp_all/          # Pre-trained features and model checkpoints
â”œâ”€â”€ best_model.pt          # Best trained model weights
â”œâ”€â”€ Fold_1_5_Epoch_Metrics.csv  # Training metrics for cross-validation
â”œâ”€â”€ README.md # Project description and instructions
â”œâ”€â”€ Tutorial.ipynb
â”œâ”€â”€ creating_labels.ipynb
â”œâ”€â”€ report_visuals.ipynb
â”œâ”€â”€ training_setup.ipynb
â”œâ”€â”€ training_setup_v2.ipynb
```

## ğŸ§ª Methodology

- **Data Preprocessing:**
  - Macenko colour normalization
  - Downsampling WSIs to 5Ã— magnification
  - Extraction of 224Ã—224 overlapping tiles
- **Baseline ViT Model:**
  - Fine-tuned using 5-fold cross-validation
- **Proposed CLIP-based Model:**
  - Textual prompts extracted using LLMs (e.g., GPT-4o, DeepSeek-R1)
  - Concept-guided hierarchical aggregation
  - Vision and text encoders from QuiltNet (trained on Quilt-1M dataset)

## ğŸ”¥ Results

| Model         | Accuracy (%) | F1-Micro (%) | AUC (%) |
|:--------------|:-------------:|:------------:|:-------:|
| ViT-B16        | 87.17         | 87.17        | 87.17   |
| QuiltNet (CLIP)| 83.93         | 83.93        | 91.76   |

The QuiltNet model showed higher AUC, indicating better overall discriminative power.

## ğŸ¥ Clinical Impact

By aligning AI predictions with expert-driven medical concepts, this method offers enhanced interpretability, better diagnostic consistency, and the potential to aid pathologists in clinical settings.

## ğŸš€ Future Work

- Expand prompt diversity to improve classification robustness.
- Acquire larger annotated datasets.

## ğŸ¤ Authors

- **Youssef Megahed** â€” MASc, Data Science, Analytics, and Artificial Intelligence at Carleton University
- **Atallah Madi** â€” MASc, Electrical and Computer Engineering at Carleton University

ğŸ“§ Contact: youssefmegahed@cmail.carleton.ca or atallahmadi@cmail.carleton.ca
