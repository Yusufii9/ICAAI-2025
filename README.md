# CSI5386 Project â€” Aligning Knowledge Concepts to Whole Slide Images for Hirschsprungâ€™s Disease Classification

## ğŸ“‹ Overview

This project proposes a novel framework that integrates expert-derived textual concepts into a CLIP-based vision-language model to guide plexus classification in histopathological whole slide images (WSIs) for Hirschsprungâ€™s Disease (HD).  
Our method aims to combine the strengths of Vision Transformers (ViTs) with clinical knowledge, improving interpretability and alignment with real-world diagnostic processes.

## âœ¨ Highlights

- **Baseline Model:** Vision Transformer (ViT) fine-tuned on histopathological tiles.
- **Proposed Model:** CLIP-based multi-modal model integrating expert prompts.
- **Dataset:** 30 WSIs from 26 patients (Childrenâ€™s Hospital of Eastern Ontario).
- **Results:**  
  - ViT achieved higher raw classification accuracy (87.17%).  
  - Proposed CLIP model achieved better AUC (91.76%), showing stronger discriminative ability.
- **Contribution:** Demonstrates how expert knowledge can enhance model reasoning in medical imaging tasks.

## ğŸ“‚ Project Structure

CSI5386---NLP/ â”œâ”€â”€ ConcepPath/ # Concept learning components â”œâ”€â”€ dataset/ # Placeholder for dataset structure â”œâ”€â”€ experiment/plexus_detection/ # Input and Output saved files â”œâ”€â”€ prompts/ # Prompt files used for CLIP model â”œâ”€â”€ reports/ # Analysis reports and visualizations â”œâ”€â”€ saved_rp_all/dataset_quilt1m_5x_224/ # Precomputed dataset features â”œâ”€â”€ Fold_1_5_Epoch_Metrics.csv # Training metrics for cross-validation â”œâ”€â”€ README.md # Project description and instructions â”œâ”€â”€ Tutorial.ipynb # Tutorial and demonstration notebook â”œâ”€â”€ creating_labels.ipynb # Label generation scripts â”œâ”€â”€ report_visuals.ipynb # Visualizing model outputs â”œâ”€â”€ training_setup.ipynb # Initial training configuration â”œâ”€â”€ training_setup_v2.ipynb # Updated training configuration


## ğŸ§ª Methodology

- **Data Preprocessing:**
  - Macenko color normalization
  - Downsampling WSIs to 5Ã— magnification
  - Extraction of 224Ã—224 overlapping tiles
- **Baseline ViT Model:**
  - Fine-tuned using 5-fold cross-validation
- **Proposed CLIP-based Model:**
  - Textual prompts extracted using LLMs (e.g., GPT-4o, DeepSeek-R1)
  - Concept-guided hierarchical aggregation
  - Vision and text encoders from QuiltNet (trained on Quilt-1M dataset)

## ğŸ”¥ Results

| Model         | Accuracy (%) | F1-Micro (%) | AUC (%) |
|:--------------|:-------------:|:------------:|:-------:|
| ViT-B16        | 87.17         | 87.17        | 87.17   |
| Quilt-1M (CLIP)| 83.93         | 83.93        | 91.76   |

The Quilt-1M model showed higher AUC, indicating better overall discriminative power.

## ğŸ¥ Clinical Impact

By aligning AI predictions with expert-driven medical concepts, this method offers enhanced interpretability, better diagnostic consistency, and potential to aid pathologists in clinical settings.

## ğŸš€ Future Work

- Expand prompt diversity to improve classification robustness.
- Acquire larger annotated datasets.
- Optimize computational efficiency for real-time applications.
- Conduct clinical validation studies.

## ğŸ¤ Authors

- **Youssef Megahed** â€” Carleton University
- **Atallah Madi** â€” Carleton University
- **Rowan Hussein** â€” University of Ottawa

ğŸ“§ Contact: youssefmegahed@cmail.carleton.ca
